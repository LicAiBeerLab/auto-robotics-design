{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from auto_robot_design.generator.restricted_generator.two_link_generator import TwoLinkGenerator, visualize_constrains\n",
    "from pymoo.decomposition.asf import ASF\n",
    "from auto_robot_design.description.builder import jps_graph2pinocchio_robot\n",
    "from auto_robot_design.description.utils import draw_joint_point\n",
    "from auto_robot_design.optimization.problems import MultiCriteriaProblem\n",
    "from auto_robot_design.optimization.optimizer import PymooOptimizer\n",
    "from auto_robot_design.pinokla.calc_criterion import ActuatedMass, EffectiveInertiaCompute, ImfCompute, ManipCompute, MovmentSurface, NeutralPoseMass, TranslationErrorMSE, ManipJacobian\n",
    "from auto_robot_design.pinokla.criterion_agregator import CriteriaAggregator\n",
    "from auto_robot_design.pinokla.criterion_math import ImfProjections\n",
    "from auto_robot_design.pinokla.default_traj import add_auxilary_points_to_trajectory, convert_x_y_to_6d_traj_xz, get_vertical_trajectory, create_simple_step_trajectory, get_workspace_trajectory\n",
    "\n",
    "from auto_robot_design.optimization.rewards.reward_base import PositioningConstrain, PositioningErrorCalculator, RewardManager\n",
    "from auto_robot_design.optimization.rewards.jacobian_and_inertia_rewards import HeavyLiftingReward, AccelerationCapability, MeanHeavyLiftingReward, MinAccelerationCapability\n",
    "from auto_robot_design.optimization.rewards.pure_jacobian_rewards import EndPointZRRReward, VelocityReward, ManipulabilityReward, ForceEllipsoidReward, ZRRReward, MinForceReward, MinManipulabilityReward,DexterityIndexReward\n",
    "from auto_robot_design.optimization.rewards.inertia_rewards import MassReward, EndPointIMFReward,ActuatedMassReward, TrajectoryIMFReward\n",
    "from auto_robot_design.description.actuators import TMotor_AK10_9, TMotor_AK60_6, TMotor_AK70_10, TMotor_AK80_64, TMotor_AK80_9\n",
    "from auto_robot_design.description.builder import ParametrizedBuilder, DetailedURDFCreatorFixedEE, jps_graph2pinocchio_robot, MIT_CHEETAH_PARAMS_DICT\n",
    "from auto_robot_design.generator.topologies.graph_manager_2l import GraphManager2L\n",
    "from auto_robot_design.optimization.saver import (\n",
    "    load_checkpoint,\n",
    ")\n",
    "from auto_robot_design.generator.topologies.bounds_preset import get_preset_by_index_with_bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topology_index = 0\n",
    "gm = get_preset_by_index_with_bounds(topology_index)\n",
    "graph = gm.get_graph(np.array([-0.001,  0.00, -0.20,  0.,  -0.5001, -0.03, -0.2]))\n",
    "\n",
    "draw_joint_point(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up builder that transforms graph into pinocchio model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "thickness = MIT_CHEETAH_PARAMS_DICT[\"thickness\"]\n",
    "actuator = MIT_CHEETAH_PARAMS_DICT[\"actuator\"]\n",
    "density = MIT_CHEETAH_PARAMS_DICT[\"density\"]\n",
    "body_density = MIT_CHEETAH_PARAMS_DICT[\"body_density\"]\n",
    "\n",
    "builder = ParametrizedBuilder(DetailedURDFCreatorFixedEE,\n",
    "                              density={\"default\": density, \"G\": body_density},\n",
    "                              thickness={\"default\": thickness, \"EE\": 0.033},\n",
    "                              actuator={\"default\": actuator},\n",
    "                              size_ground=np.array(\n",
    "                                  MIT_CHEETAH_PARAMS_DICT[\"size_ground\"]),\n",
    "                              offset_ground=MIT_CHEETAH_PARAMS_DICT[\"offset_ground_rl\"]\n",
    "                              )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Builder and criterion aggregator can both be obtained from the saved data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) trajectories\n",
    "\n",
    "workspace_trajectory = convert_x_y_to_6d_traj_xz(\n",
    "    *get_workspace_trajectory([-0.12, -0.33], 0.09, 0.24, 30, 60))\n",
    "\n",
    "\n",
    "ground_symmetric_step1 = convert_x_y_to_6d_traj_xz(*add_auxilary_points_to_trajectory(create_simple_step_trajectory(\n",
    "    starting_point=[-0.11, -0.32], step_height=0.07, step_width=0.22, n_points=50)))\n",
    "\n",
    "ground_symmetric_step2 = convert_x_y_to_6d_traj_xz(*add_auxilary_points_to_trajectory(create_simple_step_trajectory(\n",
    "    starting_point=[-0.11 + 0.025, -0.32], step_height=0.06, step_width=-2*(-0.11 + 0.025), n_points=50)))\n",
    "\n",
    "ground_symmetric_step3 = convert_x_y_to_6d_traj_xz(*add_auxilary_points_to_trajectory(create_simple_step_trajectory(\n",
    "    starting_point=[-0.11 + 2 * 0.025, -0.32], step_height=0.05, step_width=-2*(-0.11 + 2 * 0.025), n_points=50)))\n",
    "\n",
    "\n",
    "central_vertical = convert_x_y_to_6d_traj_xz(\n",
    "    *add_auxilary_points_to_trajectory(get_vertical_trajectory(-0.32, 0.075, 0, 50)))\n",
    "\n",
    "left_vertical = convert_x_y_to_6d_traj_xz(\n",
    "    *add_auxilary_points_to_trajectory(get_vertical_trajectory(-0.32, 0.065, -0.09, 50)))\n",
    "\n",
    "right_vertical = convert_x_y_to_6d_traj_xz(\n",
    "    *add_auxilary_points_to_trajectory(get_vertical_trajectory(-0.32, 0.065, 0.09, 50)))\n",
    "# 2) characteristics to be calculated\n",
    "# criteria that either calculated without any reference to points, or calculated through the aggregation of values from all points on trajectory\n",
    "dict_trajectory_criteria = {\n",
    "    \"MASS\": NeutralPoseMass(),\n",
    "}\n",
    "# criteria calculated for each point on the trajectory\n",
    "dict_point_criteria = {\n",
    "    \"Effective_Inertia\": EffectiveInertiaCompute(),\n",
    "    \"Actuated_Mass\": ActuatedMass(),\n",
    "    \"Manip_Jacobian\": ManipJacobian(MovmentSurface.XZ)\n",
    "}\n",
    "# special object that calculates the criteria for a robot and a trajectory\n",
    "crag = CriteriaAggregator(dict_point_criteria, dict_trajectory_criteria)\n",
    "\n",
    "# set the rewards and weights for the optimization task\n",
    "acceleration_capability = MinAccelerationCapability(manipulability_key='Manip_Jacobian',\n",
    "                                                    trajectory_key=\"traj_6d\", error_key=\"error\", actuated_mass_key=\"Actuated_Mass\")\n",
    "\n",
    "heavy_lifting = HeavyLiftingReward(\n",
    "    manipulability_key='Manip_Jacobian', trajectory_key=\"traj_6d\", error_key=\"error\", mass_key=\"MASS\")\n",
    "\n",
    "# set up special classes for reward calculations\n",
    "error_calculator = PositioningErrorCalculator(\n",
    "    error_key='error', jacobian_key=\"Manip_Jacobian\")\n",
    "# soft_constrain = PositioningConstrain(\n",
    "#     error_calculator=error_calculator, points=[workspace_trajectory])\n",
    "soft_constrain = PositioningConstrain(error_calculator=error_calculator, points = [ground_symmetric_step1,\n",
    "                                                                                   ground_symmetric_step2,\n",
    "                                                                                   ground_symmetric_step3,\n",
    "                                                                                   central_vertical,\n",
    "                                                                                   left_vertical,\n",
    "                                                                                   right_vertical])\n",
    "\n",
    "# manager should be filled with trajectories and rewards using the manager API\n",
    "reward_manager = RewardManager(crag=crag)\n",
    "reward_manager.add_trajectory(ground_symmetric_step1, 0)\n",
    "reward_manager.add_trajectory(ground_symmetric_step2, 1)\n",
    "reward_manager.add_trajectory(ground_symmetric_step3, 2)\n",
    "\n",
    "reward_manager.add_trajectory(central_vertical, 3)\n",
    "reward_manager.add_trajectory(left_vertical, 4)\n",
    "reward_manager.add_trajectory(right_vertical, 5)\n",
    "\n",
    "reward_manager.add_reward(acceleration_capability, 0, 1)\n",
    "reward_manager.add_reward(acceleration_capability, 1, 1)\n",
    "reward_manager.add_reward(acceleration_capability, 2, 1)\n",
    "\n",
    "reward_manager.add_reward(heavy_lifting, 3, 1)\n",
    "reward_manager.add_reward(heavy_lifting, 4, 1)\n",
    "reward_manager.add_reward(heavy_lifting, 5, 1)\n",
    "\n",
    "reward_manager.add_trajectory_aggregator([0, 1, 2], 'mean')\n",
    "reward_manager.add_trajectory_aggregator([3, 4, 5], 'mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can get a trajectory from the reward manager and calculate the trajectory following data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory = reward_manager.trajectories[0]\n",
    "\n",
    "plt.plot(trajectory[:, 0], trajectory[:, 2])\n",
    "draw_joint_point(graph)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first one should check the soft constrains on the selected trajectory for the selected mechanism. If the value is zero the mechanism can follow the selected trajectory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_robot, free_robot = jps_graph2pinocchio_robot(graph, builder=builder)\n",
    "constrain_error, results = soft_constrain.calculate_constrain_error(\n",
    "    crag, fixed_robot, free_robot)\n",
    "constrain_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(np.sum(np.diff(res_dict_fixed['q'], axis=0),axis=1))\n",
    "plt.plot(np.sum(np.abs(np.diff(results[0][2]['q'], axis=0)),axis=1))# sum of abs diffs of joint angles for adjacent points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the calculated data one can get any reward from the total reward list, even rewards that were not used in the optimization.  \n",
    "But there can appear a problem that some data are not calculated with the criterion aggregator from the optimization. Therefore for some criteria one need to create new aggregator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criteria that either calculated without any reference to points, or calculated through the aggregation of values from all points on trajectory\n",
    "dict_trajectory_criteria = {\n",
    "    \"MASS\": NeutralPoseMass(),\n",
    "    \"POS_ERR\": TranslationErrorMSE()  # MSE of deviation from the trajectory\n",
    "}\n",
    "# criteria calculated for each point on the trajectory\n",
    "dict_point_criteria = {\n",
    "    # Impact mitigation factor along the axis\n",
    "    \"IMF\": ImfCompute(ImfProjections.Z),\n",
    "    \"MANIP\": ManipCompute(MovmentSurface.XZ),\n",
    "    \"Effective_Inertia\": EffectiveInertiaCompute(),\n",
    "    \"Actuated_Mass\": ActuatedMass(),\n",
    "    \"Manip_Jacobian\": ManipJacobian(MovmentSurface.XZ)\n",
    "}\n",
    "# special object that calculates the criteria for a robot and a trajectory\n",
    "crag = CriteriaAggregator(dict_point_criteria, dict_trajectory_criteria)\n",
    "point_criteria_vector, trajectory_criteria, res_dict_fixed = crag.get_criteria_data(fixed_robot, free_robot, trajectory)\n",
    "actuator = MIT_CHEETAH_PARAMS_DICT[\"actuator\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reward - IMF at the end points of a trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_class = EndPointIMFReward(imf_key='IMF', trajectory_key=\"traj_6d\", error_key=\"error\")\n",
    "reward, reward_list = reward_class.calculate(point_criteria_vector, trajectory_criteria, res_dict_fixed, Actuator = actuator)\n",
    "reward_vector = np.array(reward_list)\n",
    "plt.plot(reward_vector)\n",
    "plt.xlabel('trajectory_point_number')\n",
    "plt.ylabel('step_reward')\n",
    "plt.title(reward_class.reward_name)\n",
    "plt.legend([f'Reward_value {reward:.2f}'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_class = MassReward(mass_key='MASS')\n",
    "reward, reward_list = reward_class.calculate(point_criteria_vector, trajectory_criteria, res_dict_fixed, Actuator = actuator)\n",
    "print(\"Mass of the robot:\", round(reward,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actuated mass is a determinant of the mass matrix and the reward is the mean value along the trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_class = ActuatedMassReward(mass_key='Actuated_Mass')\n",
    "reward, reward_list = reward_class.calculate(point_criteria_vector, trajectory_criteria, res_dict_fixed, Actuator = actuator)\n",
    "reward_vector = np.array(reward_list)\n",
    "plt.plot(reward_vector)\n",
    "plt.xlabel('trajectory_point_number')\n",
    "plt.ylabel('step_reward')\n",
    "plt.title(reward_class.reward_name)\n",
    "plt.legend([f'Reward_value {reward:.2f}'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mean Z-IMF along the trajectory.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_class = TrajectoryIMFReward(imf_key='IMF',trajectory_key=\"traj_6d\", error_key=\"error\")\n",
    "reward, reward_list = reward_class.calculate(point_criteria_vector, trajectory_criteria, res_dict_fixed, Actuator = actuator)\n",
    "reward_vector = np.array(reward_list)\n",
    "plt.plot(reward_vector)\n",
    "plt.xlabel('trajectory_point_number')\n",
    "plt.ylabel('step_reward')\n",
    "plt.title(reward_class.reward_name)\n",
    "plt.legend([f'Reward_value {reward:.2f}'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mean tangent manipulability along the trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_class = VelocityReward(manipulability_key='Manip_Jacobian',\n",
    "                                                    trajectory_key=\"traj_6d\", error_key=\"error\")\n",
    "reward, reward_list = reward_class.calculate(point_criteria_vector, trajectory_criteria, res_dict_fixed, Actuator = actuator)\n",
    "reward_vector = np.array(reward_list)\n",
    "plt.plot(reward_vector)\n",
    "plt.xlabel('trajectory_point_number')\n",
    "plt.ylabel('step_reward')\n",
    "plt.title(reward_class.reward_name)\n",
    "plt.legend([f'Reward_value {reward:.2f}'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mean determinant of the manipulability matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_class = ManipulabilityReward(manipulability_key='MANIP',\n",
    "                                                    trajectory_key=\"traj_6d\", error_key=\"error\")\n",
    "reward, reward_list = reward_class.calculate(point_criteria_vector, trajectory_criteria, res_dict_fixed, Actuator = actuator)\n",
    "reward_vector = np.array(reward_list)\n",
    "plt.plot(reward_vector)\n",
    "plt.xlabel('trajectory_point_number')\n",
    "plt.ylabel('step_reward')\n",
    "plt.title(reward_class.reward_name)\n",
    "plt.legend([f'Reward_value {reward:.2f}'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "min eigenvalue of the manipulability matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_class = MinManipulabilityReward(manipulability_key='Manip_Jacobian',\n",
    "                                                    trajectory_key=\"traj_6d\", error_key=\"error\")\n",
    "reward, reward_list = reward_class.calculate(point_criteria_vector, trajectory_criteria, res_dict_fixed, Actuator = actuator)\n",
    "reward_vector = np.array(reward_list)\n",
    "plt.plot(reward_vector)\n",
    "plt.xlabel('trajectory_point_number')\n",
    "plt.ylabel('step_reward')\n",
    "plt.title(reward_class.reward_name)\n",
    "plt.legend([f'Reward_value {reward:.2f}'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "force capability along the trajectory - the force that can be withstand from the direction of the trajectory using unit torque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_class = ForceEllipsoidReward(manipulability_key='Manip_Jacobian',\n",
    "                                                    trajectory_key=\"traj_6d\", error_key=\"error\")\n",
    "reward, reward_list = reward_class.calculate(point_criteria_vector, trajectory_criteria, res_dict_fixed, Actuator = actuator)\n",
    "reward_vector = np.array(reward_list)\n",
    "plt.plot(reward_vector)\n",
    "plt.xlabel('trajectory_point_number')\n",
    "plt.ylabel('step_reward')\n",
    "plt.title(reward_class.reward_name)\n",
    "plt.legend([f'Reward_value {reward:.2f}'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "minimal force capability which is equal to 1/max_eigenvalue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_class = MinForceReward(manipulability_key='Manip_Jacobian',\n",
    "                                                    trajectory_key=\"traj_6d\", error_key=\"error\")\n",
    "reward, reward_list = reward_class.calculate(point_criteria_vector, trajectory_criteria, res_dict_fixed, Actuator = actuator)\n",
    "reward_vector = np.array(reward_list)\n",
    "plt.plot(reward_vector)\n",
    "plt.xlabel('trajectory_point_number')\n",
    "plt.ylabel('step_reward')\n",
    "plt.title(reward_class.reward_name)\n",
    "plt.legend([f'Reward_value {reward:.2f}'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redaction ratio in z direction in the end points of the trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_class = EndPointZRRReward(manipulability_key='Manip_Jacobian',\n",
    "                                                    trajectory_key=\"traj_6d\", error_key=\"error\")\n",
    "reward, reward_list = reward_class.calculate(point_criteria_vector, trajectory_criteria, res_dict_fixed, Actuator = actuator)\n",
    "reward_vector = np.array(reward_list)\n",
    "plt.plot(reward_vector)\n",
    "plt.xlabel('trajectory_point_number')\n",
    "plt.ylabel('step_reward')\n",
    "plt.title(reward_class.reward_name)\n",
    "plt.legend([f'Reward_value {reward:.2f}'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduction ratio in z direction along the trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_class = ZRRReward(manipulability_key='Manip_Jacobian',\n",
    "                                                    trajectory_key=\"traj_6d\", error_key=\"error\")\n",
    "reward, reward_list = reward_class.calculate(point_criteria_vector, trajectory_criteria, res_dict_fixed, Actuator = actuator)\n",
    "reward_vector = np.array(reward_list)\n",
    "plt.plot(reward_vector)\n",
    "plt.xlabel('trajectory_point_number')\n",
    "plt.ylabel('step_reward')\n",
    "plt.title(reward_class.reward_name)\n",
    "plt.legend([f'Reward_value {reward:.2f}'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dexterity index min_eigenvalue/max_eigenvalue along the trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_class = DexterityIndexReward(manipulability_key='Manip_Jacobian',\n",
    "                                                    trajectory_key=\"traj_6d\", error_key=\"error\")\n",
    "reward, reward_list = reward_class.calculate(point_criteria_vector, trajectory_criteria, res_dict_fixed, Actuator = actuator)\n",
    "reward_vector = np.array(reward_list)\n",
    "plt.plot(reward_vector)\n",
    "plt.xlabel('trajectory_point_number')\n",
    "plt.ylabel('step_reward')\n",
    "plt.title(reward_class.reward_name)\n",
    "plt.legend([f'Reward_value {reward:.2f}'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weight the mechanism can withstand at certain actuator capacity. The reward is the minimum value along the trajectory, so it is the weight it can carry along the whole trajectory  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_class = HeavyLiftingReward(manipulability_key='Manip_Jacobian',mass_key='MASS',\n",
    "                                                    trajectory_key=\"traj_6d\", error_key=\"error\")\n",
    "reward, reward_list = reward_class.calculate(point_criteria_vector, trajectory_criteria, res_dict_fixed, Actuator = actuator)\n",
    "reward_vector = np.array(reward_list)\n",
    "plt.plot(reward_vector)\n",
    "plt.xlabel('trajectory_point_number')\n",
    "plt.ylabel('step_reward')\n",
    "plt.title(reward_class.reward_name)\n",
    "plt.legend([f'Reward_value {reward:.2f}'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each point the acceleration capability is an acceleration that can be achieved by stationary mechanism using certain percentage of the actuator capacity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_class = AccelerationCapability(manipulability_key='Manip_Jacobian',\n",
    "                                                    trajectory_key=\"traj_6d\", error_key=\"error\", actuated_mass_key=\"Actuated_Mass\")\n",
    "reward, reward_list = reward_class.calculate(point_criteria_vector, trajectory_criteria, res_dict_fixed, Actuator = actuator)\n",
    "reward_vector = np.array(reward_list)\n",
    "plt.plot(reward_vector)\n",
    "plt.xlabel('trajectory_point_number')\n",
    "plt.ylabel('step_reward')\n",
    "plt.title(reward_class.reward_name)\n",
    "plt.legend([f'Reward_value {reward:.2f}'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The minimum acceleration capability at the point. Evaluates the ability to move in least favored direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_class = MinAccelerationCapability(manipulability_key='Manip_Jacobian',\n",
    "                                                    trajectory_key=\"traj_6d\", error_key=\"error\", actuated_mass_key=\"Actuated_Mass\")\n",
    "reward, reward_list = reward_class.calculate(point_criteria_vector, trajectory_criteria, res_dict_fixed, Actuator = actuator)\n",
    "reward_vector = np.array(reward_list)\n",
    "plt.plot(reward_vector)\n",
    "plt.xlabel('trajectory_point_number')\n",
    "plt.ylabel('step_reward')\n",
    "plt.title(reward_class.reward_name)\n",
    "plt.legend([f'Reward_value {reward:.2f}'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variant of heavy lifting reward that evaluates the mean weight the mech can carry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_class = MeanHeavyLiftingReward(manipulability_key='Manip_Jacobian',mass_key='MASS',\n",
    "                                                    trajectory_key=\"traj_6d\", error_key=\"error\")\n",
    "reward, reward_list = reward_class.calculate(point_criteria_vector, trajectory_criteria, res_dict_fixed, Actuator = actuator)\n",
    "reward_vector = np.array(reward_list)\n",
    "plt.plot(reward_vector)\n",
    "plt.xlabel('trajectory_point_number')\n",
    "plt.ylabel('step_reward')\n",
    "plt.title(reward_class.reward_name)\n",
    "plt.legend([f'Reward_value {reward:.2f}'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All above manipulations can be applied to any trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "workspace_trajectory = convert_x_y_to_6d_traj_xz(\n",
    "    *get_workspace_trajectory([-0.12, -0.33], 0.09, 0.24, 30, 60))\n",
    "# set up special classes for reward calculations\n",
    "error_calculator = PositioningErrorCalculator(\n",
    "    error_key='error', jacobian_key=\"Manip_Jacobian\")\n",
    "soft_constrain = PositioningConstrain(\n",
    "    error_calculator=error_calculator, points=[workspace_trajectory])\n",
    "constrain_error, results = soft_constrain.calculate_constrain_error(\n",
    "    crag, fixed_robot, free_robot)\n",
    "constrain_error\n",
    "#point_criteria_vector, trajectory_criteria, res_dict_fixed = problem.rewards_and_trajectories.crag.get_criteria_data(fixed_robot, free_robot, workspace_trajectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.sum(np.abs(np.diff(results[0][2]['q'], axis=0)),axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from auto_robot_design.pinokla.criterion_agregator import calculate_quasi_static_simdata\n",
    "# from pinocchio.visualize import MeshcatVisualizer\n",
    "# import meshcat\n",
    "# viz = MeshcatVisualizer(fixed_robot.model, fixed_robot.visual_model, fixed_robot.visual_model)\n",
    "# viz.viewer = meshcat.Visualizer().open()\n",
    "# viz.clean()\n",
    "# viz.loadViewerModel()\n",
    "# calculate_quasi_static_simdata(free_robot, fixed_robot,'EE', trajectory, viz=viz)\n",
    "# pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "j_moves",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
